{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nladocsi/DS-2002-F25/blob/main/assignment_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc23963-2cc5-4f1d-8278-fb1b2026afc5",
      "metadata": {
        "id": "cfc23963-2cc5-4f1d-8278-fb1b2026afc5"
      },
      "source": [
        "## Assignment: $k$ Means Clustering\n",
        "\n",
        "### `! git clone https://github.com/ds3001f25/clustering_assignment.git`\n",
        "\n",
        "### **Do two questions in total: \"Q1+Q2\" or \"Q1+Q3\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26523935-9e8f-4377-920c-6f65605c0e31",
      "metadata": {
        "id": "26523935-9e8f-4377-920c-6f65605c0e31"
      },
      "source": [
        "**Q1.** This is a question about clustering. We want to investigate how adjusting the \"noisiness\" of the data impacts the quality of the algorithm and the difficulty of picking $k$.\n",
        "\n",
        "1. Run the code below, which creates four datasets: `df0_125`, `df0_25`, `df0_5`, `df1_0`, and `df2_0`. Each data set is created by increasing the amount of `noise` (standard deviation) around the cluster centers, from `0.125` to `0.25` to `0.5` to `1.0` to `2.0`.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def createData(noise,N=50):\n",
        "    np.random.seed(100) # Set the seed for replicability\n",
        "    # Generate (x1,x2,g) triples:\n",
        "    X1 = np.array([np.random.normal(1,noise,N),np.random.normal(1,noise,N)])\n",
        "    X2 = np.array([np.random.normal(3,noise,N),np.random.normal(2,noise,N)])\n",
        "    X3 = np.array([np.random.normal(5,noise,N),np.random.normal(3,noise,N)])\n",
        "    # Concatenate into one data frame\n",
        "    gdf1 = pd.DataFrame({'x1':X1[0,:],'x2':X1[1,:],'group':'a'})\n",
        "    gdf2 = pd.DataFrame({'x1':X2[0,:],'x2':X2[1,:],'group':'b'})\n",
        "    gdf3 = pd.DataFrame({'x1':X3[0,:],'x2':X3[1,:],'group':'c'})\n",
        "    df = pd.concat([gdf1,gdf2,gdf3],axis=0)\n",
        "    return df\n",
        "\n",
        "df0_125 = createData(0.125)\n",
        "df0_25 = createData(0.25)\n",
        "df0_5 = createData(0.5)\n",
        "df1_0 = createData(1.0)\n",
        "df2_0 = createData(2.0)\n",
        "```\n",
        "\n",
        "2. Make scatterplots of the $(X1,X2)$ points by group for each of the datasets. As the `noise` goes up from 0.125 to 2.0, what happens to the visual distinctness of the clusters?\n",
        "3. Create a scree plot for each of the datasets. Describe how the level of `noise` affects the scree plot (particularly the presence of a clear \"elbow\") and your ability to definitively select a $k$. (Pay attention to the vertical axis across plots, or put all the scree curves on a single canvas.)\n",
        "4. Explain the intuition of the elbow, using this numerical simulation as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f3ef1845",
      "metadata": {
        "id": "f3ef1845"
      },
      "outputs": [],
      "source": [
        "#1.1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "def createData(noise,N=50):\n",
        "    np.random.seed(100) # Set the seed for replicability\n",
        "    # Generate (x1,x2,g) triples:\n",
        "    X1 = np.array([np.random.normal(1,noise,N),np.random.normal(1,noise,N)])\n",
        "    X2 = np.array([np.random.normal(3,noise,N),np.random.normal(2,noise,N)])\n",
        "    X3 = np.array([np.random.normal(5,noise,N),np.random.normal(3,noise,N)])\n",
        "    # Concatenate into one data frame\n",
        "    gdf1 = pd.DataFrame({'x1':X1[0,:],'x2':X1[1,:],'group':'a'})\n",
        "    gdf2 = pd.DataFrame({'x1':X2[0,:],'x2':X2[1,:],'group':'b'})\n",
        "    gdf3 = pd.DataFrame({'x1':X3[0,:],'x2':X3[1,:],'group':'c'})\n",
        "    df = pd.concat([gdf1,gdf2,gdf3],axis=0)\n",
        "    return df\n",
        "\n",
        "df0_125 = createData(0.125)\n",
        "df0_25 = createData(0.25)\n",
        "df0_5 = createData(0.5)\n",
        "df1_0 = createData(1.0)\n",
        "df2_0 = createData(2.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2"
      ],
      "metadata": {
        "id": "0K17bhfnIwdt"
      },
      "id": "0K17bhfnIwdt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8013bcba",
      "metadata": {
        "id": "8013bcba"
      },
      "source": [
        "**Q2.** This question is a case study on clustering.\n",
        "\n",
        "1. Load the `2022 election cycle fundraising.csv` file in the `./data` folder. This has campaign finance data for the 2022 election for House and Senate candidates. We're going to focus on the total amount they raised, `Raised`, the total amount they spent, `Spent`, their available `Cash on Hand`, and their `Debts`. The variables denominated in dollars are messy and require cleaning. How do you handle it?\n",
        "2. Max-min normalize `Raised` and `Spent`. Use a scree plot to determine the optimal number of clusters for the $k$ means clustering algorithm. Make a scatter plot of `Raised` against `Spent` and hue the dots by their cluster membership. What do you see? Which politicians comprise the smallest two clusters? If necessary, look up some of these races to see how close they were.\n",
        "3. Repeat part 2, but for `Cash on Hand` and `Debts`. Compare your results with part 2. Why might this be? If necessary, look up some of these races to see how close they were.\n",
        "4. Use $k$ means clustering with all four numeric variables. How do your results compare to the previous two parts?\n",
        "5. Did the $k$-MC algorithm find useful patterns for you in analyzing the election?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62df40e5",
      "metadata": {
        "id": "62df40e5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cfc78796",
      "metadata": {
        "id": "cfc78796"
      },
      "source": [
        "**Q3.** This question is a case study on clustering.\n",
        "\n",
        "1. Load the `SIPRI Military Expenditure Database.csv` file in the `./data` folder. This has data about military spending by country. Filter the rows to select only the year 2020, and drop all rows with missing values. I ended up with 148 countries. Is any further cleaning of the variables required?\n",
        "2. Max-min normalize `Spending (2020 USD)` and `Spending per Capita`. Use a scree plot to determine the optimal number of clusters for the $k$ means clustering algorithm. Make a scatter plot of `Spending (2020 USD)` and `Spending per Capita`, and hue the dots by their cluster membership. Compute a describe table conditional on cluster membership (i.e. `.groupby(cluster).describe()`). What do you see? Where is the United States? Do you notice any patterns in the cluster membership?\n",
        "3. Repeat part 2 for `Percent of Government Spending` and `Percent of GDP`. How do your results compare to part 2?\n",
        "4. Use $k$ means clustering with all four numeric variables: `Spending (2020 USD)`, `Spending per Capita`, `Percent of Government Spending`, and `Percent of GDP`. How do your results compare to the previous two parts?\n",
        "5. Did the $k$-MC algorithm find any useful patterns for you in analyzing the spending?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cf4349",
      "metadata": {
        "id": "e4cf4349"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}